---
title: Week 8
author: Abdelrahman Jamal
---
<!--
SPDX-License-Identifier: CC-BY-SA-4.0

SPDX-FileCopyrightText: 2023 Abdelrahman Jamal <abdelrahmanjamal5565@gmail.com>
-->

*(July,19,2023)*

## Attendees:

* [Abdelrahman](https://github.com/Hero2323)
* [Ayush](https://github.com/hastagAB)
* [Gaurav](https://github.com/GMishx)
* [Kaushlendra](https://github.com/Kaushl2208)


## Updates:
- I'm finally done with clearing the Fossology dataset. The clearing results can be found [here](https://docs.google.com/spreadsheets/d/1jj_5F8bjT5a7beIp9OOIizCr37SqfeFWUiPthuEotsw/edit?usp=sharing). Green is copyright, red is a false positive, orange is unsure, which I consulted my mentors about, blue is for texts which are in another language (to easily return to them later to address them), and gray is for strings that follow the copyright format but don't contain a valid copyright (`Copyright (c) _____` for example).
- The final Fossology dataset contains around 20,000 unique strings, of which around 75% are true copyright notices, and the remaining 25% are false positives. Note that the original Fossology dataset had around 43,000 rows.
- I also started working on the copyright classification using machine learning, the code can be found [here](https://gist.github.com/Hero2323/464b1eb7321a7408613b0de3f6c11837). The following is a summary of my findings.
  * For the classical machine learning methods, I tested out SVMs, random forests, and Naive Bayes classifiers. Of all of them, random forest was the best one.
  * here are the results of the random forest model, which was trained on 80% of the Fossology dataset, tested on the remaining 20% as validation, and on the copyrights from the Tensorflow and Kubernetes datasets.
  * First, the test results on the remaining 20% of Fossology's dataset.
`
                  precision    recall  f1-score   support

           0       0.99      0.98      0.99      2870
           1       0.95      0.97      0.96      1024

    accuracy                           0.98      3894
    macro avg      0.97      0.98      0.97      3894
    weighted avg   0.98      0.98      0.98      3894
`
  * Second, the test results on the tensorflow dataset.
`

              precision    recall  f1-score   support

           0       1.00      0.98      0.99     14865
           1       0.88      0.99      0.93      1632

    accuracy                           0.99     16497
    macro avg      0.94      0.99      0.96     16497
    weighted avg   0.99      0.99      0.99     16497
`
  * Third, the test results on the kubernetes dataset.
`

              precision    recall  f1-score   support

           0       1.00      1.00      1.00     25786
           1       0.87      1.00      0.93       156

    accuracy                           1.00     25942
    macro avg      0.94      1.00      0.97     25942
    weighted avg   1.00      1.00      1.00     25942
`
  * The precision represents how accurate the model is, for example, if it says 10 strings are valid copyrights and only 5 of them are valid, then the precision is 0.5.
  * The recall represents how good the model is at remembering what it learned. For example, if the data contains 100 valid copyrights and 50 invalid ones. If the model classifies the 100 valid ones as valid and 20 of the invalid ones as valid, the recall would be one, while the precision would be 0.8 (for class 0).
  * Over all the results are promising.
- After discussing with Kaushl and Gaurav, we reached the conclusion that I should focus on recall as much as possible (realistically, the highest I'll be able to get to is around 0.95-0.99 in both recall and precision, where I can expect these results to generalize to unseen data).
- I also tested out DistilBert, which is the smallest Bert model around, and its overall results were worse than the random forest, due to the lack of data. Overall, it's unlikely that I will be using anything that size as even DistilBert is too large for this task.
- I also discussed with my mentors how we should deal with clutter removal, and after some light discussion, we expect that It will be done via NER and that I should focus on the classification task, which is the main goal of the project, to begin with.
- Finally, Gaurav was able to provide me with a dataset of 10,000 copyrights, which can be found [here](https://docs.google.com/spreadsheets/d/1nvQOz7Phx9zaxnQR22T728u6b98x8vGrkCFmdZIKvvg/edit?usp=sharing), however, they will require some light editing to be able to use them. I could also request more copyrights later if needed. These copyrights I'll use to test my current approach, and for training later on. 
- Finally, I'll keep exploring more approaches for the next three weeks, then I can move on to the de-cluttering task.

## Dataset Creation Problems and Solutions
**Problem 1**
* Creating a dataset from scratch by myself is a repetitive and time-consuming process that's prone to human error. It's very easy to mislabel something when there are more than 20,000 unique and differing rows filled with code and copyrights which are sometimes partially incorrect but still considered copyrights nonetheless.

**Solution 1** 
* Other than using scancodes, which as mentioned previously was inconsistent due to it depending on grammatical rules, I started using conditional formatting rules in google sheets to color each row depending on its label, and I used more than just 0 (copyright) and 1 (false positive). I also chose to add more labels; 2 and 3 were for when scancodes got something wrong (I intended to provide this data to my mentors to see how good scancodes truly are), however, I quickly stopped using them once I realized that scancodes required quite a complex pattern matching and even when implemented it wasn't perfect, so I'd never know if scancode truly missed this copyright or if it was due to me not matching the text scancode created with the original Fossology output. I also added label 4 for when I was unsure about classification and 5 for text in languages other than English. Lastly, I added label 6 for partially correct copyrights that I was unsure of. Each of the labels 1, 2, 4, 5, and 6 were colored light green, light red, orange, blue, and light grey respectively. As it turned out, this method was quite helpful even in the following weeks when I needed to refer to specific data rows relevant to a specific class. This method also made it ALOT easier and faster for me to work on labeling each row.

## Conclusion and further plans:
- Keep working on exploring different classifiers.
- Test out different text vectorization methods.
- Test out the potential of deep learning models vs machine learning ones.
- Make sure my methods work and **generalize well to unseen data**.