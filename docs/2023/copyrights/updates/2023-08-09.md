---
title: Week 11
author: Abdelrahman Jamal
---
<!--
SPDX-License-Identifier: CC-BY-SA-4.0

SPDX-FileCopyrightText: 2023 Abdelrahman Jamal <abdelrahmanjamal5565@gmail.com>
-->

*(August,09,2023)*

## Attendees:

* [Abdelrahman](https://github.com/Hero2323)
* [Ayush](https://github.com/hastagAB)
* [Gaurav](https://github.com/GMishx)
* [Kaushlendra](https://github.com/Kaushl2208)
* [Anupam](https://github.com/ag4ums)

## Updates:
1. This week I started by looking at my datasets and I found some mistakes in them, I corrected all of the datasets and updated them in [this](https://docs.google.com/spreadsheets/d/132NnbJT4nqb-hxPX-XRFvUWTUg9SW0-ueW2YkpykgSk/edit?usp=sharing) spreadsheet as well as added a `pred` column with my current model's prediction results. Here are some of my findings
     * I noticed that the way I was treating separate language rows was different across datasets, which was making my performance worse than it was, I corrected this by treating ALL separate language records as copyrights and then leaving it up to manual intervention afterward.
     * I also found some mistakes that I had made while annotating the datasets, I found them by having my model predict the classification of each row and then going through the results. All the mistakes were also corrected and updated in the spreadsheet mentioned before.
     * Lastly, since different languages are present in different datasets, I opted to merge all the datasets and train on them and then test on the remaining 20%.
     * Here are some statistics about the dataset now (class 0 is the copyrights)
       * Class 0 percentage & size: 75.22% - 16377 rows 
       * Class 1 percentage & size: 24.77% - 5393 rows
       * A total of 21770 rows
         * Fossology dataset percentage & size: 89.94% - 19467 rows
         * Kubernetes dataset percentage & size: 2.65% - 577 rows
         * Tensorflow dataset percentage & size: 1.14% - 249 rows
         * Fossology provided dataset #1 percentage & size: 6.78% - 1477 rows
         * Also, this week, Gaurav was able to provide me with another dataset that has 26188 unique rows. I have not had the time to label it yet so I can't say how is the split between copyrights and false positives but I expect it to be similar to our present split (75% class 0 and 25% class 1).
     * Here are my best results after making all of those changes using the TF-IDF vectorizer.
    ```
    Number of missclassifications in class 0:  52 out of a total sample of:  16377  - about  0.32 % of the class was missclassified
    Number of missclassifications in class 1:  33 out of a total sample of:  5393  - about  0.61 % of the class was missclassified
    ```
   * This is already a very good result but this was actually without any preprocessing (outside of what TF-IDf already does) and without changing any of the TF-IDF parameters.
   * To improve this, I implemented a preprocessing function that does various things including
     * replacing any digit numbers (`2023 -> DATE`)
     * replacing copyright symbols `(c), (C), Â© -> COPYRIGHTSYMBOL`
     * removing numbers
     * replacing emails to `EMAIL`
     * removing special characters and extra white spaces
     * etc.
   * that improved my best performance to around
   ```
   Number of missclassifications in class 0:  43 out of a total sample of:  16377  - about  0.26 % of the class was missclassified
   Number of missclassifications in class 1:  44 out of a total sample of:  5393  - about  0.82 % of the class was missclassified

   ```
   * Then I started experimenting with the TF-IDF vectorization parameters and that allowed me to reach my best current accuracy of
   ```
   Number of missclassifications in class 0:  27 out of a total sample of:  16377  - about  0.16 % of the class was missclassified
   Number of missclassifications in class 1:  29 out of a total sample of:  5393  - about  0.54 % of the class was missclassified
   ```
   * Finally, here are my results with a 0.99 threshold
   ```
   Number of missclassifications in class 0:  5 out of a total sample of:  16377  - about  0.03 % of the class was missclassified
   Number of missclassifications in class 1:  248 out of a total sample of:  5393  - about  4.6 % of the class was missclassified

   ```
   * **This is almost at 1 misclassification per 10,000 rows while still reducing the false positives by more than 95%**
   * The results are good, There is still potential to improve further by improving the preprocessing function and looking at what exactly gets misclassified. 
2. I was also worried about the model not generalizing properly on unseen data so I tested the model on two datasets
   * The first was the fossology-provided-2 dataset I mentioned above, but I didn't have all the dataset labeled, so I simply took all the rows labeled with 0 (copyrights) and gave them to the model to see how many would be detected. Doing so yielded these results
   ```
   Number of missclassifications in class 0:  27 out of a total sample of:  5808  - about  0.46 % of the class was missclassified
   Number of missclassifications in class 1:  0 out of a total sample of:  0  - about  100.0 % of the class was missclassified
   ```
   * Since the dataset is all 0, it makes sense that there are no correct class 1 predictions. So, it got 27 incorrect rows, but this was incorrect, going through the misclassified rows, it turned out 15 of them were supposed to be false positives so the model only got 12 wrong. Applying a 0.99 threshold gets this number down to 7
   * All in all pretty good results, however, I wanted to test on another dataset.
   * The second dataset I tested my model on was the dataset created by the authors of [this](https://doi.org/10.1587/transinf.2020EDL8089) paper, which doesn't use vectorization but uses a feature extraction method to classify an accuracy of 100% for both classes on their dataset. Their dataset is however very clean and way smaller than the data I'm working on and so it's way easier to achieve a good performance on that dataset. Here are my results on their dataset which is composed of `2146` class 0 rows and `151` class 1 rows.
   ```
   Number of missclassifications in class 0:  2 out of a total sample of:  2146  - about  0.09 % of the class was missclassified
   Number of missclassifications in class 1:  2 out of a total sample of:  151  - about  1.32 % of the class was missclassified
   ```
   * After checking the rows, I found that actually, the two misclassifications in class 1 were correct (the dataset was incorrectly annotated) and that the model only got 2 misclassifications in class 0.
   * All in all the model does generalize and with a little bit of improvement, I can achieve the goal of around or less than 1 misclassification per 1000 rows without the need for thresholding.
3. Next thing I tested out the feature extraction approach that the paper I mentioned earlier implemented.
   ```
   Number of missclassifications in class 0:  477 out of a total sample of:  16377  - about  2.91 % of the class was missclassified
   Number of missclassifications in class 1:  374 out of a total sample of:  5393  - about  6.93 % of the class was missclassified

   ```
      *  The current results are not good, however, it has one advantage over vectorization-based approaches; copyrights typically have organization names and personal names, which no matter how much data I train on, there will always be names which the model has never seen or trained on. The advantage of feature extraction is that it extracts the number of words belonging to different categories in a sentence and those counts are what goes into the model.
4.  I also tested out the LDA approach on my preprocessed data to see how I can improve my feature extraction and here are the LDA findings for the 20 most commonly found words in each class
   ```
   Class '1': copyright, license, testdata, agent, filechecksum, fossology, copy, sha, use, tests, software, master, source, file, notice, rights, code, may, md, work
   Class '0': date, copyright, copyrightsymbol, inc, software, free, foundation, reserved, com, rights, corporation, org, siemens, others, text, nathan, university, ag, lt, gt
   ```
    * This could potentially lead to more improvements in the feature extraction but Still needs more work
5. I also tried out different language detection models, one of them was Google's compact language detector v3 `cld3` as it was the best one I found when searching for one, however, it turns out that it's using the `Apache License 2.0` which is incompatible with Fossology's `GNU General Public License v2.0`. This led to trying out Spacy's language detection model as Spacy is using the `MIT License` which is compatible.
   * The initial model performance without preprocessing was terrible
   * Even after preprocessing the data, it still detects many rows which are English as not English and vise versa.
   * It classified `1478` rows as non-English but easily more than half of them are English.
6. I also created a [Github repository](https://github.com/Hero2323/Fossology-Reducing-Copyrights) to keep all of my files on in the future.

## Conclusion and further plans:
* Work on better language detection
* Work on more preprocessing features
  * Replace all names with NAME using NER
  * replace all organization names with ORG
* Explore more feature extraction methods
* Cleanup my documentation
* Cleanup and update my GitHub repository.
* Label the fossology-provided-dataset-2 with the help of my best model