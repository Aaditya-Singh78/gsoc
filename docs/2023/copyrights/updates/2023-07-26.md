---
title: Week 9
author: Abdelrahman Jamal
---
<!--
SPDX-License-Identifier: CC-BY-SA-4.0

SPDX-FileCopyrightText: 2023 Abdelrahman Jamal <abdelrahmanjamal5565@gmail.com>
-->

*(July,26,2023)*

## Attendees:

* [Abdelrahman](https://github.com/Hero2323)
* [Ayush](https://github.com/hastagAB)
* [Shaheem](https://github.com/shaheemazmalmmd)
* [Kaushlendra](https://github.com/Kaushl2208)
* [Anupam](https://github.com/ag4ums)

## Updates:
* I started by testing SVMs on some vectorization algorithms and pre-trained word embeddings. The vectorizers and embeddings tested were
  * Bag of Words (BoW)
  * Term Frequency - Inverse Document Frequency (TF-IDF)
  * GloVe (using the mean of the words of each sentence)
  * FastText
  * Sentence Transformers
  * Word2Vec
* BoW and TF-IDF were the best results and accuracy wise
* I tested GloVe embeddings of all four dimension sizes 50, 100, 200, and 300, and the results were noticeably worse than TF-IDF. The best Glove embedding (300) was around 4% worse than the TF-IDF for both classes 0 and 1. The Golve Embeddings are pre-trained and the 300-dimension embeddings are 1GB which is quite large. 
* I tried to test the pre-trained FastText embeddings but the ones I found (Wikipedia) were more than 7GB in size and I had even loaded them into memory. So, I opted to the embedder from scratch using my data. The performance was a little worse than FastText
* The rest of the embedders performed even worse.
* Here is the performance of the best TF-IDF model I had this week

```
Precision
|      |        0 |        1 |
|:-----|---------:|---------:|
| 0    | 0.991262 | 0.967086 |
| 1    | 0.97284  | 0.703488 |
| 2    | 0.945312 | 0.892562 |
| 3    | 0.991701 | 0.911765 |
| 4    | 0.995004 | 0.974809 |
| Mean | 0.979224 | 0.889942 |

Recall
|      |        0 |        1 |
|:-----|---------:|---------:|
| 0    | 0.988153 | 0.975586 |
| 1    | 0.885393 | 0.916667 |
| 2    | 0.902985 | 0.93913  |
| 3    | 0.980312 | 0.96124  |
| 4    | 0.990982 | 0.985943 |
| Mean | 0.949565 | 0.955713 |

F1-score
|      |        0 |        1 |
|:-----|---------:|---------:|
| 0    | 0.989705 | 0.971317 |
| 1    | 0.927059 | 0.796053 |
| 2    | 0.923664 | 0.915254 |
| 3    | 0.985974 | 0.935849 |
| 4    | 0.992989 | 0.980344 |
| Mean | 0.963878 | 0.919764 |
```
* 0 is the test dataset (20 % of the Fossology dataset) and the model was trained on the remaining 80%.
* 1 is the Kubernetes dataset
* 2 is the Tensorflow dataset
* 3 is the Fossology-provided-dataset-1 
* 4 is all of the previous datasets (including the training data) merged

* Overall, the reason why TF-IDF and BoW are better than more advanced methods is probably due to two reasons
  1. The relatively small size of data.
  2. The problem itself (copyright classification) is not like a normal text classification as some text will include code and other methods
  3. The lack of preprocessing, all the testing this week was done with no preprocessing of text at all
   
* Lastly, after some discussion with Anupam, we agreed that I should continue testing on the SVM due to its `predict_proba` method which gives me the probability of each SVM prediction. For instance, it can tell me how confident the model is that this text is a copyright or not and I can use that as a confidence factor and use that as a threshold, if not 99% confident in the prediction, make it so that prediction goes to class 0, which ensures that we don't miss out on any actual copyrights while increasing the false positives a little bit more. This improves recall at the expense of precision and general model accuracy, which is fine with us.

## Problems and Solutions
**Problem 1**
* I had an issue with classification reports being too large and taking too much output space while showing information that I didn't need.

**Solution 1**
* I created a function that can aggregate reports for each dataset output and can show more than two decimal points in accuracy.
* The function also shows the mean of the precision, recall, and F1-scores at the end and this shows me the relative accuracy of the model on each dataset while also not taking into account their size.

## Conclusion and further plans:
* Test out the performance of each method after preprocessing the text.
* Test out the `predict_proba` SVM method.
