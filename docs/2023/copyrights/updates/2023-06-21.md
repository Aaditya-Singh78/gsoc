---
title: Week 4
author: Abdelrahman Jamal
---
<!--
SPDX-License-Identifier: CC-BY-SA-4.0

SPDX-FileCopyrightText: 2023 Abdelrahman Jamal <abdelrahmanjamal5565@gmail.com>
-->

*(June,21,2023)*

## Attendees:

* [Abdelrahman](https://github.com/Hero2323)
* [Anupam](https://github.com/ag4ums)
* [Gaurav](https://github.com/GMishx)
* [Shaheem](https://github.com/shaheemazmalmmd)

## Updates:
- I started working on creating a dataset of copyrights. Instead of doing it manually through the Fossology UI, I thought about automating it using chat-gpt-3.5 API. I created a set of functions that go through a directory, extract all the commented text inside each file and send that text along with a prompt to the chat-gpt API telling it to return the copyright statement found in that text. It worked for the most part and I spend most of the week iterating on this process and improving it. The code can be found [here](https://gist.github.com/Hero2323/bff12400cec5ab54467ea35ba89e976f) and my results can be found [here](https://drive.google.com/drive/folders/10cvdBEWOgr2JSWqR7X7Oz0xl-Nn2VcGU?usp=drive_link).
- As it turns out, while this approach was interesting, it's not usable in this project because to correct the false positives that Fossology produces, I need to produce the dataset using Fossology and not something external.
- I was informed that there is a Fossology API that can be used to extract the copyright statements generated by Fossology and that I can use it for the dataset creation part.
- I also worked on implementing a simple LDA (Latent Dirichlet Allocation) model with two topics, copyright & no-copyright and it was somewhat successful and detecting which words and documents are associated with copyright statements. The code for this part can be found [here](https://gist.github.com/Hero2323/3e22bc0af40323d502de6f26ef2886ab)

## Problems I faced and how I solved them
**Problem 1**
* Creating a dataset from scratch by myself is a repetitive and time-consuming process that's prone to human error.

**Solution 1**
* I tried to automate the process using chatGPT which required prompt engineering efforts on my end to get semi-usable results.
  
**Problem 2**
* Which parts of the file to send to chatGPT to see if it contains copyrights?
  
**Solution 2**
* I implemented a function that extracts only the commented lines out of the most popular extensions, but it wasn't comprehensive and when it failed, I send the entire file to chatGPT which turned out to be a bad idea.
* As it turns out, Gaurav informed me that there is a [Python library under the Fossology project](https://github.com/fossology/Nirjas), Nirjas, that already does that.

## Conclusion and further plans:
- Work on creating the dataset using the Fossology API.

