---
title: Week 10
author: Abdelrahman Jamal
---
<!--
SPDX-License-Identifier: CC-BY-SA-4.0

SPDX-FileCopyrightText: 2023 Abdelrahman Jamal <abdelrahmanjamal5565@gmail.com>
-->

*(August,02,2023)*

## Attendees:

* [Abdelrahman](https://github.com/Hero2323)
* [Gaurav](https://github.com/GMishx)
* [Kaushlendra](https://github.com/Kaushl2208)
* [Anupam](https://github.com/ag4ums)

## Updates:
* I created a preprocessing function where I try different things. What I tried
  * Simply lowercase all text.
  * Replace `(c)`, `(C)`, and `Â©` with COPYRIGHT_SYMBOL
  * Applying the word_tokenize function from the popular NLTK library
  * Removing punctuation
  * Removing Stopwords
  * Lemmatizing text
  * Various combinations of the previous
* TF-IDF and BoW, the results were worse.
* For Glove, the results improved between 1 and 2% but were still worse than TF-IDF.
* FastText experienced a similar improvement to GloVe but was a little worse than GloVe.
* This applies to the rest of the embedding methods.
* I also tried applying GridSearch to the SVM parameters and the FastText parameters but for the GridSearch to output perform my manual testing, the number of combinations to test quickly grows very large so it didn't work out.
* Finally, I tested out the `predict_proba` method and set various confidence thresholds at 0.999, 0.99, 0.95, etc. I found that 0.99 was the best threshold in general.
* Here is the normal model performance without any thresholds
```
Number of missclassifications in class 0:  145 out of a total sample of:  16079  - about  0.9 % of the class was missclassified
Number of missclassifications in class 1:  81 out of a total sample of:  5691  - about  1.42 % of the class was missclassified
``` 
* Here is the performance with a 0.999 threshold
```
Number of missclassifications in class 0:  6 out of a total sample of:  16079  - about  0.04 % of the class was missclassified
Number of missclassifications in class 1:  4072 out of a total sample of:  5691  - about  71.55 % of the class was missclassified
```
* Here it is with a 0.99 threshold
```
Number of missclassifications in class 0:  27 out of a total sample of:  16079  - about  0.17 % of the class was missclassified
Number of missclassifications in class 1:  721 out of a total sample of:  5691  - about  12.67 % of the class was missclassified
```
* Here it is with a 0.95 threshold
```
Number of missclassifications in class 0:  41 out of a total sample of:  16079  - about  0.25 % of the class was missclassified
Number of missclassifications in class 1:  387 out of a total sample of:  5691  - about  6.8 % of the class was missclassified
```
* In the end, we chose the 0.99 threshold, which means I'll attempt to improve the normal model performance as much as possible then threshold in the end. This is because the error rate is around 0.17%, which could get to around 0.1% or even less after improvements, which is around or less 1 copyright misclassified per 1000 actual copyright which removes more than 90% (after improvements) of the false positives.

## Conclusion and further plans:
* Work on improving the TF-IDF performance as much as possible
  * I still have yet to try different TF-IDF parameters, so there are some improvements there
  * there are still improvements in the preprocessing function that can be made specific to our copyright classification task
* Test out an RNN model with my new processing function
* Create a GitHub repository instead of just gists for documentation
* Work on implementing a language detection mechanism to help with rows that are in a language other than English.